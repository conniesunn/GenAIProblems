# Generative AI Coding Benchmark
Generated by ChatGPT for CS 8395
## Overview

This repository contains a set of 100 unique coding problems designed to assess various aspects of Generative AI development. These problems focus on areas like text and image generation, sequence-to-sequence models, transfer learning, evaluation metrics, and many more.

## What's New and Novel?

Traditional coding assessments often focus on algorithmic skills, data structures, or web development tasks. This coding benchmark is unique in its focus on Generative AI, a rapidly evolving field with wide-ranging applications. Each problem is specifically designed to assess a particular aspect of Generative AI, such as building generative models, text generation, image synthesis, and even tackling ethical considerations in AI.

## What Does It Assess?

The benchmark is designed to assess the following:

- Understanding and implementation of generative models
- Ability to generate text and images using various techniques
- Skills in sequence-to-sequence models and their applications
- Understanding and implementation of transfer learning
- Ability to design custom evaluation metrics
- Understanding of attention mechanisms and their debugging
- Skills in data preparation and preprocessing
- Understanding of optimization techniques specific to generative models

## How to Run the Benchmark

### Prerequisites

- Python 3.x
- Necessary Python packages (e.g., TensorFlow, PyTorch, NumPy)

### Steps

1. **Clone the Repository**: Clone this repository to your local machine.

    ```bash
    git clone https://github.com/your_username/generative_ai_coding_benchmark.git
    ```

2. **Navigate to the Directory**: Go to the directory containing the problems.

    ```bash
    cd generative_ai_coding_benchmark
    ```

3. **Read the Problems**: Open the `generative_ai_unique_descriptive_coding_problems.md` file to read through the problems.

4. **Implement Solutions**: For each problem, there is a descriptive function header to guide your implementation. Replace the `parameters` in the function header with the actual parameters required for each problem.

5. **Run Tests** (Optional): If there are any tests provided or any grading criteria, run them to assess your solution.

6. **Submit**: Once you've implemented the solutions, you can submit them according to the guidelines of your assessment.

## Contributing

Feel free to contribute to this benchmark by adding more problems, tests, or grading criteria. Pull requests are welcome.

## License

This project is licensed under the MIT License.

## Acknowledgements

This benchmark was generated using OpenAI's GPT-4 and is designed to serve as a comprehensive assessment tool for Generative AI skills.
